{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a7234931-46f1-4670-846f-1c17e7c81086",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:07:39.317814Z",
     "iopub.status.busy": "2024-10-07T01:07:39.317278Z",
     "iopub.status.idle": "2024-10-07T01:07:39.324798Z",
     "shell.execute_reply": "2024-10-07T01:07:39.323772Z",
     "shell.execute_reply.started": "2024-10-07T01:07:39.317769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/pyspark/pyspark-exercises-101-pyspark-exercises-for-data-analysis/\n",
    "\n",
    "#PySpark Exercises – 101 PySpark Exercises for Data Analysis \n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_timestamp,current_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "sc= spark.sparkContext\n",
    "from pyspark.sql.functions import col, length, translate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "448e91f2-47c8-401f-9bfb-2901b6764dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T23:14:30.451951Z",
     "iopub.status.busy": "2024-10-06T23:14:30.450783Z",
     "iopub.status.idle": "2024-10-06T23:14:31.161515Z",
     "shell.execute_reply": "2024-10-06T23:14:31.160321Z",
     "shell.execute_reply.started": "2024-10-06T23:14:30.451905Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|   10|\n",
      "|    Bob|  100|\n",
      "|Charlie|   30|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+------+\n",
      "|   Name|Value|row_id|\n",
      "+-------+-----+------+\n",
      "|  Alice|   10|     0|\n",
      "|    Bob|  100|     1|\n",
      "|Charlie|   30|     2|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2. How to convert the index of a PySpark DataFrame into a column?\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 10),\n",
    "(\"Bob\", 100),\n",
    "(\"Charlie\", 30),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()\n",
    "window_spec= Window.orderBy(monotonically_increasing_id())\n",
    "df.withColumn(\"row_id\",row_number().over(window_spec)-1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee0ca294-ace5-453d-a13b-5ba8cf732076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T23:19:50.391600Z",
     "iopub.status.busy": "2024-10-06T23:19:50.389750Z",
     "iopub.status.idle": "2024-10-06T23:19:51.003583Z",
     "shell.execute_reply": "2024-10-06T23:19:51.002247Z",
     "shell.execute_reply.started": "2024-10-06T23:19:50.391538Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|Col1|Col2|\n",
      "+----+----+\n",
      "|   a|   1|\n",
      "|   b|   2|\n",
      "|   c|   3|\n",
      "|   d|   4|\n",
      "|   e|   5|\n",
      "|   f|   6|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3.  How to combine many lists to form a PySpark DataFrame?\n",
    "\n",
    "list1 = [\"a\", \"b\", \"c\", \"d\",\"e\",\"f\"]\n",
    "list2 = [1, 2, 3, 4,5,6]\n",
    "list(zip(list1,list2))\n",
    "sc= spark.sparkContext\n",
    "df= sc.parallelize(list(zip(list1,list2))).toDF([\"Col1\",\"Col2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13788d01-f1d9-4a0b-a71c-0498dd037452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T23:30:40.183344Z",
     "iopub.status.busy": "2024-10-06T23:30:40.182825Z",
     "iopub.status.idle": "2024-10-06T23:30:42.282334Z",
     "shell.execute_reply": "2024-10-06T23:30:42.281391Z",
     "shell.execute_reply.started": "2024-10-06T23:30:40.183287Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 8, 6, 7]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. How to get the items of list A not present in list B?\n",
    "\n",
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "rdd1= sc.parallelize(list_A)\n",
    "rdd2= sc.parallelize(list_B)\n",
    "result_rdd=rdd1.subtract(rdd2)\n",
    "result_rdd.collect()\n",
    "\n",
    "#5. How to get the items not common to both list A and list B?\n",
    "result_rddA=rdd1.subtract(rdd2)\n",
    "result_rddB=rdd2.subtract(rdd1)\n",
    "result_rdd2= result_rddA.union(result_rddB)\n",
    "result_rdd2.collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52c7c54a-961a-427d-aa33-12c94a1f9d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T23:46:46.385947Z",
     "iopub.status.busy": "2024-10-06T23:46:46.385455Z",
     "iopub.status.idle": "2024-10-06T23:46:46.802121Z",
     "shell.execute_reply": "2024-10-06T23:46:46.801043Z",
     "shell.execute_reply.started": "2024-10-06T23:46:46.385903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb739477-819d-417c-9250-504a8c6e3f3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T23:49:08.612421Z",
     "iopub.status.busy": "2024-10-06T23:49:08.611928Z",
     "iopub.status.idle": "2024-10-06T23:49:09.266622Z",
     "shell.execute_reply": "2024-10-06T23:49:09.265324Z",
     "shell.execute_reply.started": "2024-10-06T23:49:08.612377Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n",
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "#6. How to get the minimum, 25th percentile, median, 75th, and max of a numeric column?\n",
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()\n",
    "# Calculate percentiles\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "23dced26-16ef-46ad-b1b7-20a12d4dff89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:00:58.567597Z",
     "iopub.status.busy": "2024-10-07T00:00:58.567084Z",
     "iopub.status.idle": "2024-10-07T00:00:59.921140Z",
     "shell.execute_reply": "2024-10-07T00:00:59.919989Z",
     "shell.execute_reply.started": "2024-10-07T00:00:58.567552Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n",
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|    other|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#8. How to keep only top 2 most frequent values as it is and replace everything else as ‘Other’?\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "\n",
    "#get top 2 jobs\n",
    "\n",
    "top_2_jobs=df.groupBy(\"job\").count().orderBy(\"count\",ascending=False).limit(2)\\\n",
    "                .select(\"job\").rdd.flatMap(lambda x : x).collect()\n",
    "top_2_jobs\n",
    "df= df.withColumn(\"job\",when(col(\"job\").isin(top_2_jobs),col(\"job\")).otherwise(\"other\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3ed8f227-aa2e-40f8-9389-66412a3926d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:02:54.742479Z",
     "iopub.status.busy": "2024-10-07T00:02:54.741935Z",
     "iopub.status.idle": "2024-10-07T00:02:55.766515Z",
     "shell.execute_reply": "2024-10-07T00:02:55.765513Z",
     "shell.execute_reply.started": "2024-10-07T00:02:54.742433Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|null|\n",
      "|   B| null| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| null|null|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|null|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   B|    3|456|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9. How to Drop rows with NA values specific to a particular column?\n",
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "df.dropna(subset=[\"Value\"]).show()\n",
    "df.dropna(subset=[\"Value\",\"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3acb01bf-5756-4d00-bd14-c74cccb72f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:05:16.269134Z",
     "iopub.status.busy": "2024-10-07T00:05:16.268604Z",
     "iopub.status.idle": "2024-10-07T00:05:17.057443Z",
     "shell.execute_reply": "2024-10-07T00:05:17.056247Z",
     "shell.execute_reply.started": "2024-10-07T00:05:16.269089Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n",
      "+--------+--------+--------+\n",
      "|new_col1|new_col2|new_col3|\n",
      "+--------+--------+--------+\n",
      "|       1|       2|       3|\n",
      "|       4|       5|       6|\n",
      "+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10. How to rename columns of a PySpark DataFrame using two lists – one containing the old column names and the other containing the new column names?\n",
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "\n",
    "df.show()\n",
    "\n",
    "list(zip(old_names,new_names))\n",
    "\n",
    "for old_names,new_names in zip(old_names,new_names):\n",
    "   df=df.withColumnRenamed( old_names,new_names)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "10628115-55a6-4afd-8af2-3b4022fd12e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:17:18.163564Z",
     "iopub.status.busy": "2024-10-07T00:17:18.163055Z",
     "iopub.status.idle": "2024-10-07T00:17:18.501131Z",
     "shell.execute_reply": "2024-10-07T00:17:18.499789Z",
     "shell.execute_reply.started": "2024-10-07T00:17:18.163520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  0|    1|\n",
      "|  1|    0|\n",
      "|  2|    0|\n",
      "|  3|    1|\n",
      "|  4|    0|\n",
      "|  5|    0|\n",
      "|  6|    1|\n",
      "|  7|    0|\n",
      "|  8|    0|\n",
      "|  9|    1|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+-----+\n",
      "| id|value|index|\n",
      "+---+-----+-----+\n",
      "|  1|    0|    1|\n",
      "|  3|    1|    3|\n",
      "|  9|    1|    9|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#14. How to extract items at given positions from a column?\n",
    "df=spark.range(10).withColumn(\"value\",when(col(\"id\") % 3 == 0,1).otherwise(0))\n",
    "df.show()\n",
    "window_spec= Window.orderBy(monotonically_increasing_id())\n",
    "df= df.withColumn(\"index\",row_number().over(window_spec)-1)\n",
    "pos=[9,3,1]\n",
    "df.filter(col(\"index\").isin(pos)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e013b168-1e74-4188-b1b6-d0572a238ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:18:35.546187Z",
     "iopub.status.busy": "2024-10-07T00:18:35.545696Z",
     "iopub.status.idle": "2024-10-07T00:18:36.791779Z",
     "shell.execute_reply": "2024-10-07T00:18:36.789930Z",
     "shell.execute_reply.started": "2024-10-07T00:18:35.546143Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_3|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#15. How to stack two DataFrames vertically ?\n",
    "\n",
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()\n",
    "df_A.union(df_B).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "86391d91-5ec1-4b37-9c4f-322b49a56cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:19:47.127381Z",
     "iopub.status.busy": "2024-10-07T00:19:47.126827Z",
     "iopub.status.idle": "2024-10-07T00:19:47.936258Z",
     "shell.execute_reply": "2024-10-07T00:19:47.935153Z",
     "shell.execute_reply.started": "2024-10-07T00:19:47.127335Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#17. How to convert the first character of each element in a series to uppercase?\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()\n",
    "df.withColumn(\"name\",initcap(col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "99e2eafd-e81f-4635-b84e-07fbfb7f3dcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:21:31.071564Z",
     "iopub.status.busy": "2024-10-07T00:21:31.071044Z",
     "iopub.status.idle": "2024-10-07T00:21:32.509548Z",
     "shell.execute_reply": "2024-10-07T00:21:32.508501Z",
     "shell.execute_reply.started": "2024-10-07T00:21:31.071520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n",
      "+-------+------+------------------+-----------------+\n",
      "|summary|  name|               age|           salary|\n",
      "+-------+------+------------------+-----------------+\n",
      "|  count|     5|                 5|                5|\n",
      "|   mean|  null|              32.4|          66000.0|\n",
      "| stddev|  null|3.2093613071762426|9617.692030835675|\n",
      "|    min| James|                29|            55000|\n",
      "|    25%|  null|                30|            60000|\n",
      "|    50%|  null|                32|            65000|\n",
      "|    75%|  null|                34|            70000|\n",
      "|    max|Robert|                37|            80000|\n",
      "+-------+------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#18. How to compute summary statistics for all columns in a dataframe\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()\n",
    "summary=df.summary()\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9e69181e-05a0-4a9e-aee8-157b9e71cf1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:34:56.146238Z",
     "iopub.status.busy": "2024-10-07T00:34:56.145512Z",
     "iopub.status.idle": "2024-10-07T00:34:56.534857Z",
     "shell.execute_reply": "2024-10-07T00:34:56.533644Z",
     "shell.execute_reply.started": "2024-10-07T00:34:56.146173Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-----------+------+\n",
      "|   name|age|salary|prev_salary|  diff|\n",
      "+-------+---+------+-----------+------+\n",
      "|  James| 34| 55000|       null|     1|\n",
      "|Michael| 30| 70000|      55000| 15000|\n",
      "| Robert| 37| 60000|      70000|-10000|\n",
      "|  Maria| 29| 80000|      60000| 20000|\n",
      "|    Jen| 32| 65000|      80000|-15000|\n",
      "+-------+---+------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#20 How to compute difference of differences between consecutive numbers of a column?\n",
    "# lag of salary difference of employee\n",
    "\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "df= df.withColumn(\"id\",monotonically_increasing_id())\n",
    "window_spec= Window.orderBy(\"id\")\n",
    "\n",
    "#calc prev value of salary\n",
    "df= df.withColumn(\"prev_salary\",lag(\"salary\").over(window_spec))\n",
    "df = df.withColumn(\"diff\",\n",
    "                  when(isnull(col(\"salary\")-col(\"prev_salary\")),1\n",
    "                  ).otherwise(col(\"salary\")-col(\"prev_salary\"))).drop(\"id\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2396a2fe-9663-4c30-9beb-d3e973dc8ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T00:56:10.270836Z",
     "iopub.status.busy": "2024-10-07T00:56:10.270279Z",
     "iopub.status.idle": "2024-10-07T00:56:11.238854Z",
     "shell.execute_reply": "2024-10-07T00:56:11.237728Z",
     "shell.execute_reply.started": "2024-10-07T00:56:10.270790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|\n",
      "+----------+-----------+----------+----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|\n",
      "+----------+-----------+----------+----------+\n",
      "\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|day_of_month|week_number|day_of_year|day_of_week|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|          18|         20|        138|          5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|          31|         52|        365|          1|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#21. How to get the day of month, week number, day of year and day of week from a date strings?\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df= df.withColumn(\"date_1\",to_date(\"date_str_1\",\"yyyy-MM-dd\"))\n",
    "df= df.withColumn(\"date_2\",to_date(\"date_str_2\",\"dd MMM yyyy\"))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "60cc6c53-9edb-4cd6-9ec7-51a118fe8783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:05:15.538594Z",
     "iopub.status.busy": "2024-10-07T01:05:15.537722Z",
     "iopub.status.idle": "2024-10-07T01:05:17.334763Z",
     "shell.execute_reply": "2024-10-07T01:05:17.333684Z",
     "shell.execute_reply.started": "2024-10-07T01:05:15.538532Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|MonthYear|\n",
      "+---------+\n",
      "| May 2010|\n",
      "| Feb 2011|\n",
      "| Mar 2012|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+\n",
      "|MonthYear|      date|\n",
      "+---------+----------+\n",
      "| May 2010|2010-05-01|\n",
      "| Feb 2011|2011-02-01|\n",
      "| Mar 2012|2012-03-01|\n",
      "+---------+----------+\n",
      "\n",
      "+---------+----------+\n",
      "|MonthYear|      date|\n",
      "+---------+----------+\n",
      "| May 2010|2010-05-04|\n",
      "| Feb 2011|2011-02-04|\n",
      "| Mar 2012|2012-03-04|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#22. How to convert year-month string to dates corresponding to the 4th day of the month?\n",
    "\n",
    "df = spark.createDataFrame([('May 2010',), ('Feb 2011',), ('Mar 2012',)], ['MonthYear'])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df= df.withColumn(\"date\",to_date(\"MonthYear\",\"MMM yyyy\"))\n",
    "df.show()\n",
    "df= df.withColumn(\"date\",expr(\"date_add(date_sub(date,day(date)-1),3)\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b10d3948-8ea7-4306-85ff-23f0f43bb222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:14:58.707982Z",
     "iopub.status.busy": "2024-10-07T01:14:58.707228Z",
     "iopub.status.idle": "2024-10-07T01:14:59.664022Z",
     "shell.execute_reply": "2024-10-07T01:14:59.662963Z",
     "shell.execute_reply.started": "2024-10-07T01:14:58.707912Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Word|  tsl|lngth|\n",
      "+------+-----+-----+\n",
      "| Apple|  ppl|    3|\n",
      "|Orange|  rng|    3|\n",
      "|  Plan|  Pln|    3|\n",
      "|Python|Pythn|    5|\n",
      "| Money|  Mny|    3|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+\n",
      "|  Word|\n",
      "+------+\n",
      "| Apple|\n",
      "|Orange|\n",
      "| Money|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#23 How to filter words that contain atleast 2 vowels from a series?\n",
    "from pyspark.sql.functions import col, length, translate\n",
    "\n",
    "df = spark.createDataFrame([('Apple',), ('Orange',), ('Plan',) , ('Python',) , ('Money',)], ['Word'])\n",
    "df= df.withColumn(\"tsl\",translate('Word','aeiouAEIOU','')).\\\n",
    "        withColumn(\"lngth\",length(translate('Word','aeiouAEIOU','')))\n",
    "df.show()\n",
    "df.filter((length(\"Word\") - col(\"lngth\")) >=2).select(\"Word\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ad7f3e29-367b-40c5-b06c-5338bf1c3684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:20:27.628924Z",
     "iopub.status.busy": "2024-10-07T01:20:27.628422Z",
     "iopub.status.idle": "2024-10-07T01:20:28.357881Z",
     "shell.execute_reply": "2024-10-07T01:20:28.356769Z",
     "shell.execute_reply.started": "2024-10-07T01:20:27.628881Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|value                     |\n",
      "+--------------------------+\n",
      "|buying books at amazom.com|\n",
      "|rameses@egypt.com         |\n",
      "|matt@t.co                 |\n",
      "|narendra@modi.com         |\n",
      "+--------------------------+\n",
      "\n",
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|rameses@egypt.com|\n",
      "|        matt@t.co|\n",
      "|narendra@modi.com|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#24. How to filter valid emails from a list?\n",
    "\n",
    "# Create a list\n",
    "data = ['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com']\n",
    "\n",
    "# Convert the list to DataFrame\n",
    "df = spark.createDataFrame(data, \"string\")\n",
    "df.show(truncate =False)\n",
    "pattern = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "df_filtered = df.filter(col(\"value\").rlike(pattern))\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "aa7bacd6-7b4b-423f-a581-aa7f63083d31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:26:20.802335Z",
     "iopub.status.busy": "2024-10-07T01:26:20.801838Z",
     "iopub.status.idle": "2024-10-07T01:26:22.784326Z",
     "shell.execute_reply": "2024-10-07T01:26:22.783328Z",
     "shell.execute_reply.started": "2024-10-07T01:26:20.802279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+\n",
      "|year|quarter|region|revenue|\n",
      "+----+-------+------+-------+\n",
      "|2021|      1|    US|   5000|\n",
      "|2021|      1|    EU|   4000|\n",
      "|2021|      2|    US|   5500|\n",
      "|2021|      2|    EU|   4500|\n",
      "|2021|      3|    US|   6000|\n",
      "|2021|      3|    EU|   5000|\n",
      "|2021|      4|    US|   7000|\n",
      "|2021|      4|    EU|   6000|\n",
      "+----+-------+------+-------+\n",
      "\n",
      "+----+-------+------+------------+\n",
      "|year|quarter|region|sum(revenue)|\n",
      "+----+-------+------+------------+\n",
      "|2021|      2|    EU|        4500|\n",
      "|2021|      2|    US|        5500|\n",
      "|2021|      4|    US|        7000|\n",
      "|2021|      4|    EU|        6000|\n",
      "|2021|      1|    EU|        4000|\n",
      "|2021|      1|    US|        5000|\n",
      "|2021|      3|    EU|        5000|\n",
      "|2021|      3|    US|        6000|\n",
      "+----+-------+------+------------+\n",
      "\n",
      "+----+-------+----+----+\n",
      "|year|quarter|  EU|  US|\n",
      "+----+-------+----+----+\n",
      "|2021|      3|5000|6000|\n",
      "|2021|      4|6000|7000|\n",
      "|2021|      1|4000|5000|\n",
      "|2021|      2|4500|5500|\n",
      "+----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#25. How to Pivot PySpark DataFrame?\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "(2021, 1, \"US\", 5000),\n",
    "(2021, 1, \"EU\", 4000),\n",
    "(2021, 2, \"US\", 5500),\n",
    "(2021, 2, \"EU\", 4500),\n",
    "(2021, 3, \"US\", 6000),\n",
    "(2021, 3, \"EU\", 5000),\n",
    "(2021, 4, \"US\", 7000),\n",
    "(2021, 4, \"EU\", 6000),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"year\", \"quarter\", \"region\", \"revenue\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "df.groupBy(\"year\",\"quarter\",\"region\").sum(\"revenue\").show()\n",
    "\n",
    "pivot_df = df.groupBy(\"year\",\"quarter\").pivot(\"region\").sum(\"revenue\")\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6f1be1aa-3ee1-49a4-883f-34f796041e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T01:29:55.189229Z",
     "iopub.status.busy": "2024-10-07T01:29:55.188737Z",
     "iopub.status.idle": "2024-10-07T01:29:55.969064Z",
     "shell.execute_reply": "2024-10-07T01:29:55.968087Z",
     "shell.execute_reply.started": "2024-10-07T01:29:55.189185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|OrderID|   Product|Price|\n",
      "+-------+----------+-----+\n",
      "|   1001|    Laptop| 1000|\n",
      "|   1002|     Mouse|   50|\n",
      "|   1003|    Laptop| 1200|\n",
      "|   1004|     Mouse|   30|\n",
      "|   1005|Smartphone|  700|\n",
      "+-------+----------+-----+\n",
      "\n",
      "+----------+-----------+\n",
      "|   Product|total_price|\n",
      "+----------+-----------+\n",
      "|    Laptop|     1100.0|\n",
      "|     Mouse|       40.0|\n",
      "|Smartphone|      700.0|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#26. How to get the mean of a variable grouped by another variable?\n",
    "\n",
    "# Sample data\n",
    "data = [(\"1001\", \"Laptop\", 1000),\n",
    "(\"1002\", \"Mouse\", 50),\n",
    "(\"1003\", \"Laptop\", 1200),\n",
    "(\"1004\", \"Mouse\", 30),\n",
    "(\"1005\", \"Smartphone\", 700)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"OrderID\", \"Product\", \"Price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()\n",
    "df.groupBy(\"Product\").agg(mean(\"Price\").alias(\"total_price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "44326b69-36a0-4161-828a-da328af4eb9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T02:41:38.056664Z",
     "iopub.status.busy": "2024-10-07T02:41:38.056155Z",
     "iopub.status.idle": "2024-10-07T02:41:39.140863Z",
     "shell.execute_reply": "2024-10-07T02:41:39.139815Z",
     "shell.execute_reply.started": "2024-10-07T02:41:38.056620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|           string|\n",
      "+-----------------+\n",
      "|dbc deb abed gade|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+-----------------+\n",
      "|           string|  modified_string|\n",
      "+-----------------+-----------------+\n",
      "|dbc deb abed gade|dbccdebcabedcgade|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#28. How to replace missing spaces in a string with the least frequent character?\n",
    "#Sample DataFrame\n",
    "df = spark.createDataFrame([('dbc deb abed gade',),], [\"string\"])\n",
    "df.show()\n",
    "\n",
    "def least_freq_char_replace_spaces(str1):\n",
    "    final_str= str1.replace(\" \",\"\")\n",
    "    #print(final_str)\n",
    "    dict1={}\n",
    "    for ele in final_str:\n",
    "        for j in ele:\n",
    "            if j in dict1.keys():\n",
    "                dict1[j]+=1\n",
    "            else:\n",
    "                dict1[j]=1\n",
    "    #print(dict1)\n",
    "    #sort dict on values\n",
    "    res=dict(sorted(dict1.items(), key=lambda item: item[1]))\n",
    "    least_occur_element= list(res.keys())[0]\n",
    "    str2=str1.replace(\" \",least_occur_element)\n",
    "    return str2\n",
    "\n",
    "udf_least_freq_char_replace_spaces = udf(least_freq_char_replace_spaces, StringType())\n",
    "final_df= df.withColumn(\"modified_string\",udf_least_freq_char_replace_spaces(df.string))\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "51e26522-18d0-4d51-b293-0a816e966238",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T04:15:53.120771Z",
     "iopub.status.busy": "2024-10-07T04:15:53.120222Z",
     "iopub.status.idle": "2024-10-07T04:15:54.225854Z",
     "shell.execute_reply": "2024-10-07T04:15:54.224792Z",
     "shell.execute_reply.started": "2024-10-07T04:15:53.120717Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|null|\n",
      "|   B| null| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| null|null|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   0|    2|  2|\n",
      "+----+-----+---+\n",
      "\n",
      "True\n",
      "{'Name': 0, 'Value': 2, 'id': 2}\n"
     ]
    }
   ],
   "source": [
    "#32. How to check if a dataframe has any missing values and count of missing values in each column?\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "#missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "\n",
    "missing.show()\n",
    "#missing.collect()[0].asDict()\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "print(has_missing)\n",
    "\n",
    "missing_count = missing.collect()[0].asDict()\n",
    "print(missing_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3.4.0",
   "language": "python",
   "name": "spark-3.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
